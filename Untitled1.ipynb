{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32629,"status":"ok","timestamp":1655232468180,"user":{"displayName":"reza abbasi","userId":"16461171515024221393"},"user_tz":-270},"id":"j4LNSlD4Zlrl","outputId":"8c30486f-9fbd-47c7-f2e6-4c39d55a3432"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655232468181,"user":{"displayName":"reza abbasi","userId":"16461171515024221393"},"user_tz":-270},"id":"WLYwtXDnZ0-W","outputId":"d5125ce1-7b20-41f4-a02c-587b5691c5ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/models/MA\n","/content/drive/MyDrive/models/MA\n","Tue Jun 14 18:47:48 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["%cd '/content/drive/MyDrive/models/MA/'\n","!pwd\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Isvskv05Z02K","outputId":"ba37ee72-aec7-451d-8fcf-a919c7e91fb3"},"outputs":[{"name":"stdout","output_type":"stream","text":["model is se_resnet, loss function is MsLoss\n","device cuda:0\n","1\n","/content/drive/MyDrive/datasets/bracs\n","['s0', 's1', 's2', 's3', 's4', 's5', 's6']\n","['s0', 's1', 's2', 's3', 's4', 's5', 's6']\n","epoch 0/100\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n","  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"]},{"name":"stdout","output_type":"stream","text":["Train: [0/78250 (0%)]\tLoss: 2.024109\n","Train: [128/78250 (0%)]\tLoss: 2.032403\n","Train: [256/78250 (0%)]\tLoss: 2.017245\n","Train: [384/78250 (0%)]\tLoss: 2.026827\n","Train: [512/78250 (1%)]\tLoss: 2.017508\n","Train: [640/78250 (1%)]\tLoss: 2.017009\n","Train: [768/78250 (1%)]\tLoss: 2.018934\n","Train: [896/78250 (1%)]\tLoss: 2.018723\n","Train: [1024/78250 (1%)]\tLoss: 2.011974\n","Train: [1152/78250 (1%)]\tLoss: 2.011748\n","Train: [1280/78250 (2%)]\tLoss: 2.017451\n","Train: [1408/78250 (2%)]\tLoss: 2.024696\n","Train: [1536/78250 (2%)]\tLoss: 2.031567\n","Train: [1664/78250 (2%)]\tLoss: 2.012810\n","Train: [1792/78250 (2%)]\tLoss: 2.059814\n","Train: [1920/78250 (2%)]\tLoss: 2.023706\n","Train: [2048/78250 (3%)]\tLoss: 2.009896\n","Train: [2176/78250 (3%)]\tLoss: 2.035027\n","Train: [2304/78250 (3%)]\tLoss: 2.034814\n","Train: [2432/78250 (3%)]\tLoss: 2.027340\n","Train: [2560/78250 (3%)]\tLoss: 2.021721\n","Train: [2688/78250 (3%)]\tLoss: 2.025013\n","Train: [2816/78250 (4%)]\tLoss: 2.027335\n","Train: [2944/78250 (4%)]\tLoss: 2.039261\n","Train: [3072/78250 (4%)]\tLoss: 2.019499\n","Train: [3200/78250 (4%)]\tLoss: 2.021498\n","Train: [3328/78250 (4%)]\tLoss: 2.033445\n","Train: [3456/78250 (4%)]\tLoss: 2.026547\n","Train: [3584/78250 (5%)]\tLoss: 2.035301\n","Train: [3712/78250 (5%)]\tLoss: 2.023940\n","Train: [3840/78250 (5%)]\tLoss: 2.023946\n","Train: [3968/78250 (5%)]\tLoss: 2.008510\n","Train: [4096/78250 (5%)]\tLoss: 2.026995\n","Train: [4224/78250 (5%)]\tLoss: 2.019032\n","Train: [4352/78250 (6%)]\tLoss: 2.019986\n","Train: [4480/78250 (6%)]\tLoss: 2.025710\n","Train: [4608/78250 (6%)]\tLoss: 2.008367\n","Train: [4736/78250 (6%)]\tLoss: 2.034146\n","Train: [4864/78250 (6%)]\tLoss: 2.022716\n","Train: [4992/78250 (6%)]\tLoss: 2.023444\n","Train: [5120/78250 (7%)]\tLoss: 2.018661\n","Train: [5248/78250 (7%)]\tLoss: 2.035427\n","Train: [5376/78250 (7%)]\tLoss: 2.024732\n","Train: [5504/78250 (7%)]\tLoss: 2.030766\n","Train: [5632/78250 (7%)]\tLoss: 2.038590\n","Train: [5760/78250 (7%)]\tLoss: 2.058357\n","Train: [5888/78250 (8%)]\tLoss: 2.030061\n","Train: [6016/78250 (8%)]\tLoss: 2.030215\n","Train: [6144/78250 (8%)]\tLoss: 2.028423\n","Train: [6272/78250 (8%)]\tLoss: 2.017259\n","Train: [6400/78250 (8%)]\tLoss: 2.019849\n","Train: [6528/78250 (8%)]\tLoss: 2.008335\n","Train: [6656/78250 (9%)]\tLoss: 2.019670\n","Train: [6784/78250 (9%)]\tLoss: 2.026236\n","Train: [6912/78250 (9%)]\tLoss: 2.023810\n","Train: [7040/78250 (9%)]\tLoss: 2.013084\n","Train: [7168/78250 (9%)]\tLoss: 2.012505\n","Train: [7296/78250 (9%)]\tLoss: 2.018162\n","Train: [7424/78250 (9%)]\tLoss: 2.023947\n","Train: [7552/78250 (10%)]\tLoss: 2.039140\n","Train: [7680/78250 (10%)]\tLoss: 2.024062\n","Train: [7808/78250 (10%)]\tLoss: 2.003277\n","Train: [7936/78250 (10%)]\tLoss: 2.015819\n","Train: [8064/78250 (10%)]\tLoss: 2.021833\n","Train: [8192/78250 (10%)]\tLoss: 2.026348\n","Train: [8320/78250 (11%)]\tLoss: 2.005276\n","Train: [8448/78250 (11%)]\tLoss: 2.009990\n","Train: [8576/78250 (11%)]\tLoss: 2.011963\n","Train: [8704/78250 (11%)]\tLoss: 2.024073\n","Train: [8832/78250 (11%)]\tLoss: 2.027521\n","Train: [8960/78250 (11%)]\tLoss: 2.008315\n","Train: [9088/78250 (12%)]\tLoss: 2.027827\n","Train: [9216/78250 (12%)]\tLoss: 2.009331\n","Train: [9344/78250 (12%)]\tLoss: 2.026604\n","Train: [9472/78250 (12%)]\tLoss: 2.027767\n","Train: [9600/78250 (12%)]\tLoss: 2.004245\n","Train: [9728/78250 (12%)]\tLoss: 2.019299\n","Train: [9856/78250 (13%)]\tLoss: 2.014661\n","Train: [9984/78250 (13%)]\tLoss: 2.031534\n","Train: [10112/78250 (13%)]\tLoss: 2.027282\n","Train: [10240/78250 (13%)]\tLoss: 2.018733\n","Train: [10368/78250 (13%)]\tLoss: 2.006264\n","Train: [10496/78250 (13%)]\tLoss: 2.013296\n","Train: [10624/78250 (14%)]\tLoss: 2.020842\n","Train: [10752/78250 (14%)]\tLoss: 2.021439\n","Train: [10880/78250 (14%)]\tLoss: 2.036192\n","Train: [11008/78250 (14%)]\tLoss: 2.003933\n","Train: [11136/78250 (14%)]\tLoss: 2.003575\n","Train: [11264/78250 (14%)]\tLoss: 2.020386\n","Train: [11392/78250 (15%)]\tLoss: 2.009477\n","Train: [11520/78250 (15%)]\tLoss: 2.003348\n","Train: [11648/78250 (15%)]\tLoss: 2.013116\n","Train: [11776/78250 (15%)]\tLoss: 2.001261\n","Train: [11904/78250 (15%)]\tLoss: 2.000033\n","Train: [12032/78250 (15%)]\tLoss: 1.997836\n","Train: [12160/78250 (16%)]\tLoss: 2.006577\n","Train: [12288/78250 (16%)]\tLoss: 2.007152\n","Train: [12416/78250 (16%)]\tLoss: 2.006064\n","Train: [12544/78250 (16%)]\tLoss: 2.001290\n","Train: [12672/78250 (16%)]\tLoss: 2.018504\n","Train: [12800/78250 (16%)]\tLoss: 2.004479\n","Train: [12928/78250 (17%)]\tLoss: 2.006983\n","Train: [13056/78250 (17%)]\tLoss: 2.014353\n","Train: [13184/78250 (17%)]\tLoss: 2.002679\n","Train: [13312/78250 (17%)]\tLoss: 2.011431\n","Train: [13440/78250 (17%)]\tLoss: 1.992925\n","Train: [13568/78250 (17%)]\tLoss: 1.976092\n","Train: [13696/78250 (18%)]\tLoss: 1.975917\n","Train: [13824/78250 (18%)]\tLoss: 1.967808\n","Train: [13952/78250 (18%)]\tLoss: 1.964385\n","Train: [14080/78250 (18%)]\tLoss: 1.984154\n","Train: [14208/78250 (18%)]\tLoss: 1.964102\n","Train: [14336/78250 (18%)]\tLoss: 1.960871\n","Train: [14464/78250 (18%)]\tLoss: 1.987935\n","Train: [14592/78250 (19%)]\tLoss: 1.958745\n","Train: [14720/78250 (19%)]\tLoss: 1.961532\n","Train: [14848/78250 (19%)]\tLoss: 1.954522\n","Train: [14976/78250 (19%)]\tLoss: 1.955797\n","Train: [15104/78250 (19%)]\tLoss: 1.950517\n","Train: [15232/78250 (19%)]\tLoss: 1.952650\n","Train: [15360/78250 (20%)]\tLoss: 1.941105\n","Train: [15488/78250 (20%)]\tLoss: 1.951092\n","Train: [15616/78250 (20%)]\tLoss: 1.950628\n","Train: [15744/78250 (20%)]\tLoss: 1.949796\n","Train: [15872/78250 (20%)]\tLoss: 1.932601\n","Train: [16000/78250 (20%)]\tLoss: 1.943185\n","Train: [16128/78250 (21%)]\tLoss: 1.931830\n","Train: [16256/78250 (21%)]\tLoss: 1.937333\n","Train: [16384/78250 (21%)]\tLoss: 1.927417\n","Train: [16512/78250 (21%)]\tLoss: 1.936918\n","Train: [16640/78250 (21%)]\tLoss: 1.937242\n","Train: [16768/78250 (21%)]\tLoss: 1.934385\n","Train: [16896/78250 (22%)]\tLoss: 1.914885\n","Train: [17024/78250 (22%)]\tLoss: 1.926806\n","Train: [17152/78250 (22%)]\tLoss: 1.947679\n","Train: [17280/78250 (22%)]\tLoss: 1.925830\n","Train: [17408/78250 (22%)]\tLoss: 1.935062\n","Train: [17536/78250 (22%)]\tLoss: 1.910649\n","Train: [17664/78250 (23%)]\tLoss: 1.934349\n","Train: [17792/78250 (23%)]\tLoss: 1.940086\n","Train: [17920/78250 (23%)]\tLoss: 1.937326\n","Train: [18048/78250 (23%)]\tLoss: 1.949657\n","Train: [18176/78250 (23%)]\tLoss: 1.944608\n","Train: [18304/78250 (23%)]\tLoss: 1.922215\n","Train: [18432/78250 (24%)]\tLoss: 1.926500\n","Train: [18560/78250 (24%)]\tLoss: 1.926248\n","Train: [18688/78250 (24%)]\tLoss: 1.921526\n","Train: [18816/78250 (24%)]\tLoss: 1.927278\n","Train: [18944/78250 (24%)]\tLoss: 1.938979\n","Train: [19072/78250 (24%)]\tLoss: 1.938031\n","Train: [19200/78250 (25%)]\tLoss: 1.931033\n","Train: [19328/78250 (25%)]\tLoss: 1.950535\n","Train: [19456/78250 (25%)]\tLoss: 1.939876\n","Train: [19584/78250 (25%)]\tLoss: 1.940583\n","Train: [19712/78250 (25%)]\tLoss: 1.921929\n","Train: [19840/78250 (25%)]\tLoss: 1.931986\n","Train: [19968/78250 (26%)]\tLoss: 1.918588\n","Train: [20096/78250 (26%)]\tLoss: 1.966924\n","Train: [20224/78250 (26%)]\tLoss: 1.954815\n","Train: [20352/78250 (26%)]\tLoss: 1.937207\n","Train: [20480/78250 (26%)]\tLoss: 1.942347\n","Train: [20608/78250 (26%)]\tLoss: 1.929355\n","Train: [20736/78250 (27%)]\tLoss: 1.931834\n","Train: [20864/78250 (27%)]\tLoss: 1.924746\n","Train: [20992/78250 (27%)]\tLoss: 1.920923\n","Train: [21120/78250 (27%)]\tLoss: 1.955956\n","Train: [21248/78250 (27%)]\tLoss: 1.936321\n","Train: [21376/78250 (27%)]\tLoss: 1.925923\n","Train: [21504/78250 (27%)]\tLoss: 1.924650\n","Train: [21632/78250 (28%)]\tLoss: 1.953165\n","Train: [21760/78250 (28%)]\tLoss: 1.958976\n","Train: [21888/78250 (28%)]\tLoss: 1.950345\n","Train: [22016/78250 (28%)]\tLoss: 1.950767\n","Train: [22144/78250 (28%)]\tLoss: 1.927772\n","Train: [22272/78250 (28%)]\tLoss: 1.938185\n","Train: [22400/78250 (29%)]\tLoss: 1.950921\n","Train: [22528/78250 (29%)]\tLoss: 1.925817\n","Train: [22656/78250 (29%)]\tLoss: 1.939493\n","Train: [22784/78250 (29%)]\tLoss: 1.937716\n","Train: [22912/78250 (29%)]\tLoss: 1.929163\n","Train: [23040/78250 (29%)]\tLoss: 1.945708\n","Train: [23168/78250 (30%)]\tLoss: 1.953516\n","Train: [23296/78250 (30%)]\tLoss: 1.936027\n","Train: [23424/78250 (30%)]\tLoss: 1.911874\n","Train: [23552/78250 (30%)]\tLoss: 1.927853\n","Train: [23680/78250 (30%)]\tLoss: 1.934382\n","Train: [23808/78250 (30%)]\tLoss: 1.909922\n","Train: [23936/78250 (31%)]\tLoss: 1.948975\n","Train: [24064/78250 (31%)]\tLoss: 1.998839\n","Train: [24192/78250 (31%)]\tLoss: 1.919856\n","Train: [24320/78250 (31%)]\tLoss: 1.929342\n","Train: [24448/78250 (31%)]\tLoss: 1.923318\n","Train: [24576/78250 (31%)]\tLoss: 1.941577\n","Train: [24704/78250 (32%)]\tLoss: 1.924804\n","Train: [24832/78250 (32%)]\tLoss: 1.928976\n","Train: [24960/78250 (32%)]\tLoss: 1.946733\n","Train: [25088/78250 (32%)]\tLoss: 1.926428\n","Train: [25216/78250 (32%)]\tLoss: 1.952327\n","Train: [25344/78250 (32%)]\tLoss: 1.928994\n","Train: [25472/78250 (33%)]\tLoss: 1.940836\n","Train: [25600/78250 (33%)]\tLoss: 1.924223\n","Train: [25728/78250 (33%)]\tLoss: 1.936092\n","Train: [25856/78250 (33%)]\tLoss: 1.932228\n","Train: [25984/78250 (33%)]\tLoss: 1.946526\n","Train: [26112/78250 (33%)]\tLoss: 1.941975\n","Train: [26240/78250 (34%)]\tLoss: 1.970341\n","Train: [26368/78250 (34%)]\tLoss: 1.918361\n","Train: [26496/78250 (34%)]\tLoss: 1.922877\n","Train: [26624/78250 (34%)]\tLoss: 1.923476\n","Train: [26752/78250 (34%)]\tLoss: 1.940697\n","Train: [26880/78250 (34%)]\tLoss: 1.949881\n","Train: [27008/78250 (35%)]\tLoss: 1.939275\n","Train: [27136/78250 (35%)]\tLoss: 1.943563\n","Train: [27264/78250 (35%)]\tLoss: 1.939295\n","Train: [27392/78250 (35%)]\tLoss: 1.951836\n","Train: [27520/78250 (35%)]\tLoss: 1.931607\n","Train: [27648/78250 (35%)]\tLoss: 1.945234\n","Train: [27776/78250 (36%)]\tLoss: 1.919743\n","Train: [27904/78250 (36%)]\tLoss: 1.941248\n","Train: [28032/78250 (36%)]\tLoss: 1.919647\n","Train: [28160/78250 (36%)]\tLoss: 1.934447\n","Train: [28288/78250 (36%)]\tLoss: 1.934045\n","Train: [28416/78250 (36%)]\tLoss: 1.938131\n","Train: [28544/78250 (36%)]\tLoss: 1.926426\n","Train: [28672/78250 (37%)]\tLoss: 1.942545\n","Train: [28800/78250 (37%)]\tLoss: 1.937645\n","Train: [28928/78250 (37%)]\tLoss: 1.931923\n","Train: [29056/78250 (37%)]\tLoss: 1.941759\n","Train: [29184/78250 (37%)]\tLoss: 1.941454\n","Train: [29312/78250 (37%)]\tLoss: 1.962834\n","Train: [29440/78250 (38%)]\tLoss: 1.935811\n","Train: [29568/78250 (38%)]\tLoss: 1.939890\n","Train: [29696/78250 (38%)]\tLoss: 1.943662\n","Train: [29824/78250 (38%)]\tLoss: 1.931576\n","Train: [29952/78250 (38%)]\tLoss: 1.915872\n","Train: [30080/78250 (38%)]\tLoss: 1.950769\n","Train: [30208/78250 (39%)]\tLoss: 1.927056\n","Train: [30336/78250 (39%)]\tLoss: 1.922189\n","Train: [30464/78250 (39%)]\tLoss: 1.936367\n","Train: [30592/78250 (39%)]\tLoss: 1.945529\n","Train: [30720/78250 (39%)]\tLoss: 1.982988\n","Train: [30848/78250 (39%)]\tLoss: 1.935482\n","Train: [30976/78250 (40%)]\tLoss: 1.927782\n","Train: [31104/78250 (40%)]\tLoss: 1.931602\n","Train: [31232/78250 (40%)]\tLoss: 1.925502\n","Train: [31360/78250 (40%)]\tLoss: 1.919179\n","Train: [31488/78250 (40%)]\tLoss: 1.916103\n","Train: [31616/78250 (40%)]\tLoss: 1.958496\n","Train: [31744/78250 (41%)]\tLoss: 1.916674\n","Train: [31872/78250 (41%)]\tLoss: 1.929205\n","Train: [32000/78250 (41%)]\tLoss: 1.929437\n","Train: [32128/78250 (41%)]\tLoss: 1.939853\n","Train: [32256/78250 (41%)]\tLoss: 1.942539\n","Train: [32384/78250 (41%)]\tLoss: 1.927754\n","Train: [32512/78250 (42%)]\tLoss: 1.933567\n","Train: [32640/78250 (42%)]\tLoss: 1.923665\n","Train: [32768/78250 (42%)]\tLoss: 1.954988\n","Train: [32896/78250 (42%)]\tLoss: 1.935742\n","Train: [33024/78250 (42%)]\tLoss: 1.933888\n","Train: [33152/78250 (42%)]\tLoss: 1.944891\n","Train: [33280/78250 (43%)]\tLoss: 1.939867\n","Train: [33408/78250 (43%)]\tLoss: 1.949557\n","Train: [33536/78250 (43%)]\tLoss: 1.921696\n","Train: [33664/78250 (43%)]\tLoss: 1.916993\n","Train: [33792/78250 (43%)]\tLoss: 1.953706\n","Train: [33920/78250 (43%)]\tLoss: 1.925900\n","Train: [34048/78250 (44%)]\tLoss: 1.914722\n","Train: [34176/78250 (44%)]\tLoss: 1.926240\n","Train: [34304/78250 (44%)]\tLoss: 1.938646\n","Train: [34432/78250 (44%)]\tLoss: 1.927381\n","Train: [34560/78250 (44%)]\tLoss: 1.950176\n","Train: [34688/78250 (44%)]\tLoss: 1.924431\n","Train: [34816/78250 (45%)]\tLoss: 1.921300\n","Train: [34944/78250 (45%)]\tLoss: 1.937187\n","Train: [35072/78250 (45%)]\tLoss: 1.926807\n","Train: [35200/78250 (45%)]\tLoss: 1.920051\n","Train: [35328/78250 (45%)]\tLoss: 1.937124\n","Train: [35456/78250 (45%)]\tLoss: 1.921652\n","Train: [35584/78250 (45%)]\tLoss: 1.923899\n","Train: [35712/78250 (46%)]\tLoss: 1.913678\n","Train: [35840/78250 (46%)]\tLoss: 1.924998\n","Train: [35968/78250 (46%)]\tLoss: 1.924121\n","Train: [36096/78250 (46%)]\tLoss: 1.957462\n","Train: [36224/78250 (46%)]\tLoss: 1.928114\n","Train: [36352/78250 (46%)]\tLoss: 1.940714\n","Train: [36480/78250 (47%)]\tLoss: 1.943763\n","Train: [36608/78250 (47%)]\tLoss: 1.932401\n","Train: [36736/78250 (47%)]\tLoss: 1.946654\n","Train: [36864/78250 (47%)]\tLoss: 1.934318\n","Train: [36992/78250 (47%)]\tLoss: 1.931650\n","Train: [37120/78250 (47%)]\tLoss: 1.922802\n","Train: [37248/78250 (48%)]\tLoss: 1.954933\n","Train: [37376/78250 (48%)]\tLoss: 1.929135\n","Train: [37504/78250 (48%)]\tLoss: 1.917312\n","Train: [37632/78250 (48%)]\tLoss: 1.949023\n","Train: [37760/78250 (48%)]\tLoss: 1.927772\n","Train: [37888/78250 (48%)]\tLoss: 1.947594\n","Train: [38016/78250 (49%)]\tLoss: 1.939239\n","Train: [38144/78250 (49%)]\tLoss: 1.931666\n","Train: [38272/78250 (49%)]\tLoss: 1.951693\n","Train: [38400/78250 (49%)]\tLoss: 1.924171\n","Train: [38528/78250 (49%)]\tLoss: 1.975453\n","Train: [38656/78250 (49%)]\tLoss: 1.928069\n","Train: [38784/78250 (50%)]\tLoss: 1.933190\n","Train: [38912/78250 (50%)]\tLoss: 1.941840\n","Train: [39040/78250 (50%)]\tLoss: 1.924796\n","Train: [39168/78250 (50%)]\tLoss: 1.957902\n","Train: [39296/78250 (50%)]\tLoss: 2.001616\n","Train: [39424/78250 (50%)]\tLoss: 1.944271\n","Train: [39552/78250 (51%)]\tLoss: 1.951159\n","Train: [39680/78250 (51%)]\tLoss: 1.947643\n","Train: [39808/78250 (51%)]\tLoss: 1.920695\n","Train: [39936/78250 (51%)]\tLoss: 1.935722\n","Train: [40064/78250 (51%)]\tLoss: 1.925580\n","Train: [40192/78250 (51%)]\tLoss: 1.924519\n","Train: [40320/78250 (52%)]\tLoss: 1.946173\n","Train: [40448/78250 (52%)]\tLoss: 1.926745\n","Train: [40576/78250 (52%)]\tLoss: 1.917360\n","Train: [40704/78250 (52%)]\tLoss: 1.952407\n","Train: [40832/78250 (52%)]\tLoss: 1.937462\n","Train: [40960/78250 (52%)]\tLoss: 1.922650\n","Train: [41088/78250 (53%)]\tLoss: 1.919916\n","Train: [41216/78250 (53%)]\tLoss: 1.927192\n","Train: [41344/78250 (53%)]\tLoss: 1.918895\n","Train: [41472/78250 (53%)]\tLoss: 1.939628\n","Train: [41600/78250 (53%)]\tLoss: 1.955999\n","Train: [41728/78250 (53%)]\tLoss: 1.945569\n","Train: [41856/78250 (54%)]\tLoss: 1.946554\n","Train: [41984/78250 (54%)]\tLoss: 1.920435\n","Train: [42112/78250 (54%)]\tLoss: 1.939073\n","Train: [42240/78250 (54%)]\tLoss: 1.933299\n","Train: [42368/78250 (54%)]\tLoss: 1.938052\n","Train: [42496/78250 (54%)]\tLoss: 1.927603\n","Train: [42624/78250 (55%)]\tLoss: 1.947830\n","Train: [42752/78250 (55%)]\tLoss: 1.931059\n","Train: [42880/78250 (55%)]\tLoss: 1.916989\n","Train: [43008/78250 (55%)]\tLoss: 1.946535\n","Train: [43136/78250 (55%)]\tLoss: 1.924175\n","Train: [43264/78250 (55%)]\tLoss: 1.937361\n","Train: [43392/78250 (55%)]\tLoss: 1.936714\n","Train: [43520/78250 (56%)]\tLoss: 1.943814\n","Train: [43648/78250 (56%)]\tLoss: 1.935032\n","Train: [43776/78250 (56%)]\tLoss: 1.941623\n","Train: [43904/78250 (56%)]\tLoss: 1.968681\n","Train: [44032/78250 (56%)]\tLoss: 1.939071\n","Train: [44160/78250 (56%)]\tLoss: 1.927567\n","Train: [44288/78250 (57%)]\tLoss: 1.918215\n","Train: [44416/78250 (57%)]\tLoss: 1.933066\n","Train: [44544/78250 (57%)]\tLoss: 1.929937\n","Train: [44672/78250 (57%)]\tLoss: 1.929456\n","Train: [44800/78250 (57%)]\tLoss: 1.921084\n","Train: [44928/78250 (57%)]\tLoss: 1.941907\n","Train: [45056/78250 (58%)]\tLoss: 1.957340\n","Train: [45184/78250 (58%)]\tLoss: 1.931517\n","Train: [45312/78250 (58%)]\tLoss: 1.927931\n","Train: [45440/78250 (58%)]\tLoss: 1.919089\n","Train: [45568/78250 (58%)]\tLoss: 1.929545\n","Train: [45696/78250 (58%)]\tLoss: 1.920199\n","Train: [45824/78250 (59%)]\tLoss: 1.934933\n","Train: [45952/78250 (59%)]\tLoss: 1.923870\n","Train: [46080/78250 (59%)]\tLoss: 1.927559\n","Train: [46208/78250 (59%)]\tLoss: 1.962804\n","Train: [46336/78250 (59%)]\tLoss: 1.978762\n","Train: [46464/78250 (59%)]\tLoss: 1.950440\n","Train: [46592/78250 (60%)]\tLoss: 1.932414\n","Train: [46720/78250 (60%)]\tLoss: 1.923534\n","Train: [46848/78250 (60%)]\tLoss: 1.929327\n","Train: [46976/78250 (60%)]\tLoss: 1.920564\n","Train: [47104/78250 (60%)]\tLoss: 1.969635\n","Train: [47232/78250 (60%)]\tLoss: 1.958175\n","Train: [47360/78250 (61%)]\tLoss: 1.947132\n","Train: [47488/78250 (61%)]\tLoss: 1.918989\n","Train: [47616/78250 (61%)]\tLoss: 1.958607\n","Train: [47744/78250 (61%)]\tLoss: 1.917094\n","Train: [47872/78250 (61%)]\tLoss: 1.931249\n","Train: [48000/78250 (61%)]\tLoss: 1.934167\n","Train: [48128/78250 (62%)]\tLoss: 1.919727\n","Train: [48256/78250 (62%)]\tLoss: 1.955008\n","Train: [48384/78250 (62%)]\tLoss: 1.931407\n","Train: [48512/78250 (62%)]\tLoss: 1.978509\n","Train: [48640/78250 (62%)]\tLoss: 1.928963\n","Train: [48768/78250 (62%)]\tLoss: 1.939202\n","Train: [48896/78250 (63%)]\tLoss: 1.944976\n","Train: [49024/78250 (63%)]\tLoss: 1.929589\n","Train: [49152/78250 (63%)]\tLoss: 1.932210\n","Train: [49280/78250 (63%)]\tLoss: 1.932581\n","Train: [49408/78250 (63%)]\tLoss: 1.952451\n","Train: [49536/78250 (63%)]\tLoss: 1.934940\n","Train: [49664/78250 (64%)]\tLoss: 1.937132\n","Train: [49792/78250 (64%)]\tLoss: 1.919550\n","Train: [49920/78250 (64%)]\tLoss: 1.973389\n","Train: [50048/78250 (64%)]\tLoss: 1.926712\n","Train: [50176/78250 (64%)]\tLoss: 1.928230\n","Train: [50304/78250 (64%)]\tLoss: 1.944636\n","Train: [50432/78250 (64%)]\tLoss: 1.949658\n","Train: [50560/78250 (65%)]\tLoss: 1.936978\n","Train: [50688/78250 (65%)]\tLoss: 1.930081\n","Train: [50816/78250 (65%)]\tLoss: 1.915470\n","Train: [50944/78250 (65%)]\tLoss: 1.932710\n","Train: [51072/78250 (65%)]\tLoss: 1.930313\n","Train: [51200/78250 (65%)]\tLoss: 1.931374\n","Train: [51328/78250 (66%)]\tLoss: 1.947262\n","Train: [51456/78250 (66%)]\tLoss: 1.940168\n","Train: [51584/78250 (66%)]\tLoss: 1.944142\n","Train: [51712/78250 (66%)]\tLoss: 1.940361\n","Train: [51840/78250 (66%)]\tLoss: 1.928947\n","Train: [51968/78250 (66%)]\tLoss: 1.926587\n","Train: [52096/78250 (67%)]\tLoss: 1.925861\n","Train: [52224/78250 (67%)]\tLoss: 1.931553\n","Train: [52352/78250 (67%)]\tLoss: 1.939146\n","Train: [52480/78250 (67%)]\tLoss: 1.928867\n","Train: [52608/78250 (67%)]\tLoss: 1.923163\n","Train: [52736/78250 (67%)]\tLoss: 1.945546\n","Train: [52864/78250 (68%)]\tLoss: 1.929871\n","Train: [52992/78250 (68%)]\tLoss: 1.936915\n","Train: [53120/78250 (68%)]\tLoss: 1.932811\n","Train: [53248/78250 (68%)]\tLoss: 1.934764\n","Train: [53376/78250 (68%)]\tLoss: 1.944324\n","Train: [53504/78250 (68%)]\tLoss: 1.933215\n","Train: [53632/78250 (69%)]\tLoss: 1.924691\n","Train: [53760/78250 (69%)]\tLoss: 1.921964\n","Train: [53888/78250 (69%)]\tLoss: 1.942101\n","Train: [54016/78250 (69%)]\tLoss: 1.949735\n","Train: [54144/78250 (69%)]\tLoss: 1.932413\n","Train: [54272/78250 (69%)]\tLoss: 1.944455\n","Train: [54400/78250 (70%)]\tLoss: 1.946669\n","Train: [54528/78250 (70%)]\tLoss: 1.929270\n","Train: [54656/78250 (70%)]\tLoss: 1.933367\n","Train: [54784/78250 (70%)]\tLoss: 1.926209\n","Train: [54912/78250 (70%)]\tLoss: 1.945805\n","Train: [55040/78250 (70%)]\tLoss: 1.934003\n","Train: [55168/78250 (71%)]\tLoss: 1.934591\n","Train: [55296/78250 (71%)]\tLoss: 1.938982\n","Train: [55424/78250 (71%)]\tLoss: 1.923573\n","Train: [55552/78250 (71%)]\tLoss: 1.919194\n","Train: [55680/78250 (71%)]\tLoss: 1.952165\n","Train: [55808/78250 (71%)]\tLoss: 1.924049\n","Train: [55936/78250 (72%)]\tLoss: 1.924735\n","Train: [56064/78250 (72%)]\tLoss: 1.923528\n","Train: [56192/78250 (72%)]\tLoss: 1.915610\n","Train: [56320/78250 (72%)]\tLoss: 1.925950\n","Train: [56448/78250 (72%)]\tLoss: 1.920430\n","Train: [56576/78250 (72%)]\tLoss: 1.945146\n","Train: [56704/78250 (73%)]\tLoss: 1.921845\n","Train: [56832/78250 (73%)]\tLoss: 1.931754\n","Train: [56960/78250 (73%)]\tLoss: 1.933494\n","Train: [57088/78250 (73%)]\tLoss: 1.925890\n","Train: [57216/78250 (73%)]\tLoss: 1.946113\n","Train: [57344/78250 (73%)]\tLoss: 1.929515\n","Train: [57472/78250 (73%)]\tLoss: 1.939422\n","Train: [57600/78250 (74%)]\tLoss: 1.912180\n","Train: [57728/78250 (74%)]\tLoss: 1.935513\n","Train: [57856/78250 (74%)]\tLoss: 1.916984\n","Train: [57984/78250 (74%)]\tLoss: 1.934983\n","Train: [58112/78250 (74%)]\tLoss: 1.952915\n","Train: [58240/78250 (74%)]\tLoss: 1.942705\n","Train: [58368/78250 (75%)]\tLoss: 1.914135\n","Train: [58496/78250 (75%)]\tLoss: 1.946566\n","Train: [58624/78250 (75%)]\tLoss: 1.977494\n","Train: [58752/78250 (75%)]\tLoss: 1.938549\n","Train: [58880/78250 (75%)]\tLoss: 1.922581\n","Train: [59008/78250 (75%)]\tLoss: 1.928084\n","Train: [59136/78250 (76%)]\tLoss: 1.946904\n","Train: [59264/78250 (76%)]\tLoss: 1.936633\n","Train: [59392/78250 (76%)]\tLoss: 1.912448\n","Train: [59520/78250 (76%)]\tLoss: 1.934560\n","Train: [59648/78250 (76%)]\tLoss: 1.965040\n","Train: [59776/78250 (76%)]\tLoss: 1.956532\n","Train: [59904/78250 (77%)]\tLoss: 1.967884\n","Train: [60032/78250 (77%)]\tLoss: 1.935778\n","Train: [60160/78250 (77%)]\tLoss: 1.945796\n","Train: [60288/78250 (77%)]\tLoss: 1.942990\n","Train: [60416/78250 (77%)]\tLoss: 1.926044\n","Train: [60544/78250 (77%)]\tLoss: 1.935739\n","Train: [60672/78250 (78%)]\tLoss: 1.949012\n","Train: [60800/78250 (78%)]\tLoss: 1.928232\n","Train: [60928/78250 (78%)]\tLoss: 1.941326\n","Train: [61056/78250 (78%)]\tLoss: 1.924943\n","Train: [61184/78250 (78%)]\tLoss: 1.924007\n","Train: [61312/78250 (78%)]\tLoss: 1.922241\n","Train: [61440/78250 (79%)]\tLoss: 1.935871\n","Train: [61568/78250 (79%)]\tLoss: 1.912837\n","Train: [61696/78250 (79%)]\tLoss: 1.928951\n","Train: [61824/78250 (79%)]\tLoss: 1.958498\n","Train: [61952/78250 (79%)]\tLoss: 1.921518\n","Train: [62080/78250 (79%)]\tLoss: 1.915838\n","Train: [62208/78250 (80%)]\tLoss: 1.928256\n","Train: [62336/78250 (80%)]\tLoss: 1.922650\n","Train: [62464/78250 (80%)]\tLoss: 1.927009\n","Train: [62592/78250 (80%)]\tLoss: 1.945597\n","Train: [62720/78250 (80%)]\tLoss: 1.917175\n","Train: [62848/78250 (80%)]\tLoss: 1.939709\n","Train: [62976/78250 (81%)]\tLoss: 1.969429\n","Train: [63104/78250 (81%)]\tLoss: 1.952125\n","Train: [63232/78250 (81%)]\tLoss: 1.930132\n","Train: [63360/78250 (81%)]\tLoss: 1.936780\n","Train: [63488/78250 (81%)]\tLoss: 1.934038\n","Train: [63616/78250 (81%)]\tLoss: 1.939668\n","Train: [63744/78250 (82%)]\tLoss: 1.925664\n","Train: [63872/78250 (82%)]\tLoss: 1.926242\n","Train: [64000/78250 (82%)]\tLoss: 1.921629\n","Train: [64128/78250 (82%)]\tLoss: 1.930496\n","Train: [64256/78250 (82%)]\tLoss: 1.933267\n","Train: [64384/78250 (82%)]\tLoss: 1.918374\n","Train: [64512/78250 (82%)]\tLoss: 1.921621\n","Train: [64640/78250 (83%)]\tLoss: 1.935024\n","Train: [64768/78250 (83%)]\tLoss: 1.946945\n","Train: [64896/78250 (83%)]\tLoss: 1.909513\n","Train: [65024/78250 (83%)]\tLoss: 1.927096\n","Train: [65152/78250 (83%)]\tLoss: 1.946976\n","Train: [65280/78250 (83%)]\tLoss: 1.929008\n","Train: [65408/78250 (84%)]\tLoss: 1.937262\n","Train: [65536/78250 (84%)]\tLoss: 1.930806\n","Train: [65664/78250 (84%)]\tLoss: 1.922891\n","Train: [65792/78250 (84%)]\tLoss: 1.929245\n","Train: [65920/78250 (84%)]\tLoss: 1.928189\n","Train: [66048/78250 (84%)]\tLoss: 1.910712\n","Train: [66176/78250 (85%)]\tLoss: 1.932874\n","Train: [66304/78250 (85%)]\tLoss: 1.943584\n","Train: [66432/78250 (85%)]\tLoss: 1.949845\n","Train: [66560/78250 (85%)]\tLoss: 1.921974\n","Train: [66688/78250 (85%)]\tLoss: 1.919768\n","Train: [66816/78250 (85%)]\tLoss: 1.927885\n","Train: [66944/78250 (86%)]\tLoss: 1.922821\n","Train: [67072/78250 (86%)]\tLoss: 1.943382\n","Train: [67200/78250 (86%)]\tLoss: 1.946195\n","Train: [67328/78250 (86%)]\tLoss: 1.959266\n","Train: [67456/78250 (86%)]\tLoss: 1.915440\n","Train: [67584/78250 (86%)]\tLoss: 1.932817\n","Train: [67712/78250 (87%)]\tLoss: 1.928443\n","Train: [67840/78250 (87%)]\tLoss: 1.929034\n","Train: [67968/78250 (87%)]\tLoss: 1.937375\n","Train: [68096/78250 (87%)]\tLoss: 1.923736\n","Train: [68224/78250 (87%)]\tLoss: 1.961237\n","Train: [68352/78250 (87%)]\tLoss: 1.909796\n","Train: [68480/78250 (88%)]\tLoss: 1.939276\n","Train: [68608/78250 (88%)]\tLoss: 1.925887\n","Train: [68736/78250 (88%)]\tLoss: 1.910367\n","Train: [68864/78250 (88%)]\tLoss: 1.937949\n","Train: [68992/78250 (88%)]\tLoss: 1.946807\n","Train: [69120/78250 (88%)]\tLoss: 1.943144\n","Train: [69248/78250 (89%)]\tLoss: 1.930592\n","Train: [69376/78250 (89%)]\tLoss: 1.936736\n","Train: [69504/78250 (89%)]\tLoss: 1.947967\n","Train: [69632/78250 (89%)]\tLoss: 2.000982\n","Train: [69760/78250 (89%)]\tLoss: 1.910048\n","Train: [69888/78250 (89%)]\tLoss: 1.933501\n","Train: [70016/78250 (90%)]\tLoss: 1.925693\n","Train: [70144/78250 (90%)]\tLoss: 1.941949\n","Train: [70272/78250 (90%)]\tLoss: 1.936550\n","Train: [70400/78250 (90%)]\tLoss: 1.926228\n","Train: [70528/78250 (90%)]\tLoss: 1.923194\n","Train: [70656/78250 (90%)]\tLoss: 1.938169\n","Train: [70784/78250 (91%)]\tLoss: 1.917592\n","Train: [70912/78250 (91%)]\tLoss: 1.934348\n","Train: [71040/78250 (91%)]\tLoss: 1.945121\n","Train: [71168/78250 (91%)]\tLoss: 1.911597\n","Train: [71296/78250 (91%)]\tLoss: 1.917795\n","Train: [71424/78250 (91%)]\tLoss: 1.921367\n","Train: [71552/78250 (91%)]\tLoss: 1.926099\n","Train: [71680/78250 (92%)]\tLoss: 1.917974\n","Train: [71808/78250 (92%)]\tLoss: 1.933828\n","Train: [71936/78250 (92%)]\tLoss: 1.932852\n","Train: [72064/78250 (92%)]\tLoss: 1.936340\n","Train: [72192/78250 (92%)]\tLoss: 1.915971\n","Train: [72320/78250 (92%)]\tLoss: 1.918674\n","Train: [72448/78250 (93%)]\tLoss: 1.928406\n","Train: [72576/78250 (93%)]\tLoss: 1.936866\n","Train: [72704/78250 (93%)]\tLoss: 1.940005\n","Train: [72832/78250 (93%)]\tLoss: 1.911473\n","Train: [72960/78250 (93%)]\tLoss: 1.921417\n","Train: [73088/78250 (93%)]\tLoss: 1.915789\n","Train: [73216/78250 (94%)]\tLoss: 1.919316\n","Train: [73344/78250 (94%)]\tLoss: 1.926899\n","Train: [73472/78250 (94%)]\tLoss: 1.916848\n","Train: [73600/78250 (94%)]\tLoss: 1.938896\n","Train: [73728/78250 (94%)]\tLoss: 1.937701\n","Train: [73856/78250 (94%)]\tLoss: 1.931299\n","Train: [73984/78250 (95%)]\tLoss: 1.916107\n","Train: [74112/78250 (95%)]\tLoss: 1.929987\n","Train: [74240/78250 (95%)]\tLoss: 1.925123\n","Train: [74368/78250 (95%)]\tLoss: 1.937674\n","Train: [74496/78250 (95%)]\tLoss: 1.933813\n","Train: [74624/78250 (95%)]\tLoss: 1.926680\n","Train: [74752/78250 (96%)]\tLoss: 1.912317\n","Train: [74880/78250 (96%)]\tLoss: 1.920765\n","Train: [75008/78250 (96%)]\tLoss: 1.908031\n","Train: [75136/78250 (96%)]\tLoss: 1.931618\n","Train: [75264/78250 (96%)]\tLoss: 1.938548\n","Train: [75392/78250 (96%)]\tLoss: 1.932963\n","Train: [75520/78250 (97%)]\tLoss: 1.937308\n","Train: [75648/78250 (97%)]\tLoss: 1.925681\n","Train: [75776/78250 (97%)]\tLoss: 1.941140\n","Train: [75904/78250 (97%)]\tLoss: 1.918357\n","Train: [76032/78250 (97%)]\tLoss: 1.913109\n","Train: [76160/78250 (97%)]\tLoss: 1.952494\n","Train: [76288/78250 (98%)]\tLoss: 1.925245\n","Train: [76416/78250 (98%)]\tLoss: 1.909704\n","Train: [76544/78250 (98%)]\tLoss: 1.964165\n","Train: [76672/78250 (98%)]\tLoss: 1.950218\n","Train: [76800/78250 (98%)]\tLoss: 1.932546\n","Train: [76928/78250 (98%)]\tLoss: 1.942713\n","Train: [77056/78250 (99%)]\tLoss: 1.918685\n","Train: [77184/78250 (99%)]\tLoss: 1.943209\n","Train: [77312/78250 (99%)]\tLoss: 1.929015\n","Train: [77440/78250 (99%)]\tLoss: 1.915619\n","Train: [77568/78250 (99%)]\tLoss: 1.939471\n","Train: [77696/78250 (99%)]\tLoss: 1.900871\n","Train: [77824/78250 (100%)]\tLoss: 1.918880\n","Train: [77952/78250 (100%)]\tLoss: 1.939491\n","Train: [78080/78250 (100%)]\tLoss: 1.928259\n","total loss 1.949950\n"]}],"source":["import sys\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms \n","import torch.optim as optim\n","from torch.optim import lr_scheduler \n","from ABE_M_model import ABE_M \n","from my_model import se_resnet50\n","from resnet import resnet50\n","from dataset import SingleData\n","from torch.utils.data import DataLoader\n","from loss_func import ABE_loss, ContrastiveLoss, Ms_loss\n","from torchsummary import summary\n","\n","from torchvision.transforms import InterpolationMode\n","\n","def get_data_list(data_path, ratio=0.1):\n","    img_list = []\n","    for root, dirs, files in os.walk(data_path):\n","        if files == []:\n","            class_name = dirs\n","        elif dirs == []:\n","            for f in files:\n","                img_path = os.path.join(root, f)\n","                img_list.append(img_path)\n","\n","    np.random.seed(1)\n","    train_img_list = np.random.choice(img_list, size=int(len(img_list)*(1-ratio)), replace=False)\n","    #print(img_list, train_img_list)\n","    eval_img_list = list(set(img_list) - set(train_img_list))\n","    print(data_path)\n","    return class_name, train_img_list, eval_img_list \n","\n","def train_epoch(train_loader, model, loss_fn, optimizer, device):\n","    model.train()\n","    losses = []\n","    total_loss = 0\n","    for batch_idx, (data, target, _) in enumerate(train_loader):\n","        data = data.to(device)\n","        target = target.to(device)\n","\n","        optimizer.zero_grad()\n","        output = model(data)\n","        target = target\n","\n","        loss_outputs = loss_fn(output, target)\n","\n","        losses.append(loss_outputs.item())\n","        total_loss += loss_outputs.item()\n","        if loss_outputs.requires_grad is True:\n","            loss_outputs.backward()\n","            optimizer.step()\n","\n","        if batch_idx % 2 == 0:\n","            message = 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(batch_idx * target.size(0), len(train_loader.dataset), 100. * batch_idx / len(train_loader), np.mean(losses))\n","            print(message)\n","            losses = []\n","\n","    print('total loss {:.6f}'.format(total_loss/(batch_idx+1)))\n","\n","\n","def eval_epoch(eval_loader, model, loss_fn, device, best, model_name, loss_name):\n","    with torch.no_grad():\n","        model.eval()\n","        val_loss = 0\n","        for batch_idx, (data, target, _) in enumerate(eval_loader):\n","            data = data.to(device)\n","            target = target.to(device)\n","\n","            output = model(data)\n","\n","            loss_outputs = loss_fn(output, target)\n","            #print(loss_outputs)\n","            val_loss += loss_outputs.item()\n","\n","    print('val loss {:.6f}'.format(val_loss/(batch_idx+1)))\n","    if best \u003e val_loss/(batch_idx+1):\n","        best = val_loss/(batch_idx+1)\n","        if torch.cuda.device_count() \u003e 1:\n","            torch.save(model.module.state_dict(), '/content/drive/MyDrive/models/MA/bracs/{}_{}_{:.4f}.pth'.format(model_name, loss_name, best))\n","        else:\n","            torch.save(model.state_dict(), '/content/drive/MyDrive/models/MA/bracs/{}_{}_{:.4f}.pth'.format(model_name, loss_name, best))\n","    return best \n","\n","\n","if __name__ == '__main__':\n"," \n","    torch.cuda.empty_cache()\n","    # arg_len = len(sys.argv)\n","    # if arg_len != 3:\n","    #     raise Exception(\"Invalid argvs!\")\n","    # model_name = sys.argv[1]\n","    # loss_name = sys.argv[2]\n","    \n","    model_name = 'se_resnet'\n","    loss_name = 'MsLoss'\n","    print('model is {}, loss function is {}'.format(model_name, loss_name))\n","\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","    # device = torch.device('cpu')\n","    print('device',device)\n","    epochs = 100\n","    batch_size = 64   # 64\n","    lr = 0.001\n","    #num_learner = 4\n","    model_dict = {'ABE_M':ABE_M, 'se_resnet':se_resnet50, 'resnet50':resnet50}\n","    loss_dict = {'ABELoss':ABE_loss, 'ContrastiveLoss':ContrastiveLoss, 'MsLoss':Ms_loss}\n","\n","\n","    #model = model_dict[model_name](attention=False) #################### remove attention\n","    model = model_dict[model_name]()\n","    print(torch.cuda.device_count())\n","    if torch.cuda.device_count() \u003e 1:\n","        print(\"Let's use {} GPUs!\".format(torch.cuda.device_count()))\n","        model = nn.DataParallel(model)\n","    model.to(device)\n","    \n","    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n","    #optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    loss_fn = loss_dict[loss_name]() ########################################\n","\n","    scheduler = lr_scheduler.StepLR(optimizer, step_size=50)\n","    \n","    #data_path = 'D:\\\\University\\\\training-\\\\train'\n","    data_path = '/content/drive/MyDrive/datasets/bracs'\n","    \n","    class_name, train_img_list, eval_img_list = get_data_list(data_path)\n","\n","    train_transform = transforms.Compose([ \n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomVerticalFlip(),\n","        transforms.RandomCrop(224),\n","        transforms.ToTensor()\n","    ])\n","    eval_transform = transforms.Compose([\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor()\n","    ])\n","    train_transform2 = transforms.Compose([ \n","                                          transforms.Resize((224,224),interpolation=InterpolationMode.BICUBIC),\n","                                          transforms.ColorJitter(brightness=.5, saturation=.25, hue=.1, contrast=.5),\n","                                          transforms.RandomAffine(10, (0.05, 0.05), fill=(255, 255, 255)),\n","                                          transforms.RandomHorizontalFlip(.5),\n","                                          transforms.RandomVerticalFlip(.5),\n","                                         transforms.ToTensor()])\n","    \n","    #========================== single data ================================\n","    torch.cuda.empty_cache()\n","    train_dataset = SingleData(class_name, train_img_list, train_transform)\n","    eval_dataset = SingleData(class_name, eval_img_list, eval_transform)\n","    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","    eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","    \n","    best = 10000\n","    for epoch in range(epochs):\n","        torch.cuda.empty_cache()\n","        print('epoch {}/{}'.format(epoch, epochs))\n","        \n","        train_epoch(train_dataloader, model, loss_fn, optimizer, device)\n","        scheduler.step() \n","        # with torch.no_grad():\n","        best = eval_epoch(eval_dataloader, model, loss_fn, device, best, model_name, loss_name)\n","\n","        \n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"U4s2x6jG3Q6A"},"outputs":[{"name":"stdout","output_type":"stream","text":["model is se_resnet, loss function is MsLoss\n","device cuda:0\n","1\n","/content/drive/MyDrive/datasets/bracs\n","['s0', 's1', 's2', 's3', 's4', 's5', 's6']\n","['s0', 's1', 's2', 's3', 's4', 's5', 's6']\n","epoch 0/100\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n","  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"]},{"name":"stdout","output_type":"stream","text":["Train: [0/78250 (0%)]\tLoss: 1.926420\n","Train: [128/78250 (0%)]\tLoss: 1.921873\n","Train: [256/78250 (0%)]\tLoss: 1.914977\n","Train: [384/78250 (0%)]\tLoss: 1.925278\n","Train: [512/78250 (1%)]\tLoss: 1.927652\n","Train: [640/78250 (1%)]\tLoss: 1.951454\n","Train: [768/78250 (1%)]\tLoss: 1.922007\n","Train: [896/78250 (1%)]\tLoss: 1.929384\n","Train: [1024/78250 (1%)]\tLoss: 1.936297\n","Train: [1152/78250 (1%)]\tLoss: 1.913355\n","Train: [1280/78250 (2%)]\tLoss: 1.930614\n","Train: [1408/78250 (2%)]\tLoss: 1.920830\n","Train: [1536/78250 (2%)]\tLoss: 1.925698\n","Train: [1664/78250 (2%)]\tLoss: 1.937575\n","Train: [1792/78250 (2%)]\tLoss: 1.927186\n","Train: [1920/78250 (2%)]\tLoss: 1.929587\n","Train: [2048/78250 (3%)]\tLoss: 1.943607\n","Train: [2176/78250 (3%)]\tLoss: 1.923413\n","Train: [2304/78250 (3%)]\tLoss: 1.944888\n","Train: [2432/78250 (3%)]\tLoss: 1.943725\n","Train: [2560/78250 (3%)]\tLoss: 1.935466\n","Train: [2688/78250 (3%)]\tLoss: 1.924074\n","Train: [2816/78250 (4%)]\tLoss: 1.917243\n","Train: [2944/78250 (4%)]\tLoss: 1.939519\n","Train: [3072/78250 (4%)]\tLoss: 1.912006\n","Train: [3200/78250 (4%)]\tLoss: 1.928652\n","Train: [3328/78250 (4%)]\tLoss: 1.937289\n","Train: [3456/78250 (4%)]\tLoss: 1.961901\n","Train: [3584/78250 (5%)]\tLoss: 1.912412\n","Train: [3712/78250 (5%)]\tLoss: 1.915075\n","Train: [3840/78250 (5%)]\tLoss: 1.908747\n","Train: [3968/78250 (5%)]\tLoss: 1.926859\n","Train: [4096/78250 (5%)]\tLoss: 1.928285\n","Train: [4224/78250 (5%)]\tLoss: 1.920116\n","Train: [4352/78250 (6%)]\tLoss: 1.940544\n","Train: [4480/78250 (6%)]\tLoss: 1.939533\n","Train: [4608/78250 (6%)]\tLoss: 1.928797\n","Train: [4736/78250 (6%)]\tLoss: 1.938244\n","Train: [4864/78250 (6%)]\tLoss: 1.943525\n","Train: [4992/78250 (6%)]\tLoss: 1.933665\n","Train: [5120/78250 (7%)]\tLoss: 1.928510\n","Train: [5248/78250 (7%)]\tLoss: 1.930223\n","Train: [5376/78250 (7%)]\tLoss: 1.934636\n","Train: [5504/78250 (7%)]\tLoss: 1.924505\n","Train: [5632/78250 (7%)]\tLoss: 1.894194\n","Train: [5760/78250 (7%)]\tLoss: 1.938876\n","Train: [5888/78250 (8%)]\tLoss: 1.910108\n","Train: [6016/78250 (8%)]\tLoss: 1.967435\n","Train: [6144/78250 (8%)]\tLoss: 1.941713\n","Train: [6272/78250 (8%)]\tLoss: 1.931277\n","Train: [6400/78250 (8%)]\tLoss: 1.928208\n","Train: [6528/78250 (8%)]\tLoss: 1.928362\n","Train: [6656/78250 (9%)]\tLoss: 1.929628\n","Train: [6784/78250 (9%)]\tLoss: 1.960936\n","Train: [6912/78250 (9%)]\tLoss: 1.923396\n","Train: [7040/78250 (9%)]\tLoss: 1.922322\n","Train: [7168/78250 (9%)]\tLoss: 1.912387\n","Train: [7296/78250 (9%)]\tLoss: 1.912279\n","Train: [7424/78250 (9%)]\tLoss: 1.929013\n","Train: [7552/78250 (10%)]\tLoss: 1.944582\n","Train: [7680/78250 (10%)]\tLoss: 1.907602\n","Train: [7808/78250 (10%)]\tLoss: 1.928157\n","Train: [7936/78250 (10%)]\tLoss: 1.944262\n","Train: [8064/78250 (10%)]\tLoss: 1.943522\n","Train: [8192/78250 (10%)]\tLoss: 1.946875\n","Train: [8320/78250 (11%)]\tLoss: 1.952215\n","Train: [8448/78250 (11%)]\tLoss: 1.922232\n","Train: [8576/78250 (11%)]\tLoss: 1.911765\n","Train: [8704/78250 (11%)]\tLoss: 1.960854\n","Train: [8832/78250 (11%)]\tLoss: 1.927302\n","Train: [8960/78250 (11%)]\tLoss: 1.952258\n","Train: [9088/78250 (12%)]\tLoss: 1.929858\n","Train: [9216/78250 (12%)]\tLoss: 1.938155\n","Train: [9344/78250 (12%)]\tLoss: 1.929415\n","Train: [9472/78250 (12%)]\tLoss: 1.975783\n","Train: [9600/78250 (12%)]\tLoss: 1.939164\n","Train: [9728/78250 (12%)]\tLoss: 1.922446\n","Train: [9856/78250 (13%)]\tLoss: 1.916429\n","Train: [9984/78250 (13%)]\tLoss: 1.923484\n","Train: [10112/78250 (13%)]\tLoss: 1.931867\n","Train: [10240/78250 (13%)]\tLoss: 1.922982\n","Train: [10368/78250 (13%)]\tLoss: 1.945665\n","Train: [10496/78250 (13%)]\tLoss: 1.903794\n","Train: [10624/78250 (14%)]\tLoss: 1.938535\n","Train: [10752/78250 (14%)]\tLoss: 1.941990\n","Train: [10880/78250 (14%)]\tLoss: 1.911148\n","Train: [11008/78250 (14%)]\tLoss: 1.914920\n","Train: [11136/78250 (14%)]\tLoss: 1.934707\n","Train: [11264/78250 (14%)]\tLoss: 1.940126\n","Train: [11392/78250 (15%)]\tLoss: 1.913166\n","Train: [11520/78250 (15%)]\tLoss: 1.934554\n","Train: [11648/78250 (15%)]\tLoss: 1.931114\n","Train: [11776/78250 (15%)]\tLoss: 1.931474\n","Train: [11904/78250 (15%)]\tLoss: 1.934238\n","Train: [12032/78250 (15%)]\tLoss: 1.966313\n","Train: [12160/78250 (16%)]\tLoss: 1.939674\n","Train: [12288/78250 (16%)]\tLoss: 1.929618\n","Train: [12416/78250 (16%)]\tLoss: 1.915109\n","Train: [12544/78250 (16%)]\tLoss: 1.914953\n","Train: [12672/78250 (16%)]\tLoss: 1.923093\n","Train: [12800/78250 (16%)]\tLoss: 1.938806\n","Train: [12928/78250 (17%)]\tLoss: 1.939481\n","Train: [13056/78250 (17%)]\tLoss: 1.938669\n","Train: [13184/78250 (17%)]\tLoss: 1.935777\n","Train: [13312/78250 (17%)]\tLoss: 1.926312\n","Train: [13440/78250 (17%)]\tLoss: 1.954406\n","Train: [13568/78250 (17%)]\tLoss: 1.918507\n","Train: [13696/78250 (18%)]\tLoss: 1.923130\n","Train: [13824/78250 (18%)]\tLoss: 1.963920\n","Train: [13952/78250 (18%)]\tLoss: 1.918434\n","Train: [14080/78250 (18%)]\tLoss: 1.935238\n","Train: [14208/78250 (18%)]\tLoss: 1.929019\n","Train: [14336/78250 (18%)]\tLoss: 1.936897\n","Train: [14464/78250 (18%)]\tLoss: 1.942510\n","Train: [14592/78250 (19%)]\tLoss: 1.928779\n","Train: [14720/78250 (19%)]\tLoss: 1.931949\n","Train: [14848/78250 (19%)]\tLoss: 1.929769\n","Train: [14976/78250 (19%)]\tLoss: 1.936015\n","Train: [15104/78250 (19%)]\tLoss: 1.914842\n","Train: [15232/78250 (19%)]\tLoss: 1.941216\n","Train: [15360/78250 (20%)]\tLoss: 1.947401\n","Train: [15488/78250 (20%)]\tLoss: 1.936330\n","Train: [15616/78250 (20%)]\tLoss: 1.938848\n","Train: [15744/78250 (20%)]\tLoss: 1.933169\n","Train: [15872/78250 (20%)]\tLoss: 1.953932\n","Train: [16000/78250 (20%)]\tLoss: 1.925779\n","Train: [16128/78250 (21%)]\tLoss: 1.921883\n","Train: [16256/78250 (21%)]\tLoss: 1.917955\n","Train: [16384/78250 (21%)]\tLoss: 1.944030\n","Train: [16512/78250 (21%)]\tLoss: 1.931539\n","Train: [16640/78250 (21%)]\tLoss: 1.913227\n","Train: [16768/78250 (21%)]\tLoss: 1.931481\n","Train: [16896/78250 (22%)]\tLoss: 1.916142\n","Train: [17024/78250 (22%)]\tLoss: 1.928007\n","Train: [17152/78250 (22%)]\tLoss: 1.945766\n","Train: [17280/78250 (22%)]\tLoss: 1.919362\n","Train: [17408/78250 (22%)]\tLoss: 1.916884\n","Train: [17536/78250 (22%)]\tLoss: 1.930650\n","Train: [17664/78250 (23%)]\tLoss: 1.943788\n","Train: [17792/78250 (23%)]\tLoss: 1.930922\n","Train: [17920/78250 (23%)]\tLoss: 1.919256\n","Train: [18048/78250 (23%)]\tLoss: 1.921153\n","Train: [18176/78250 (23%)]\tLoss: 1.945639\n","Train: [18304/78250 (23%)]\tLoss: 1.974359\n","Train: [18432/78250 (24%)]\tLoss: 1.931950\n","Train: [18560/78250 (24%)]\tLoss: 1.923186\n","Train: [18688/78250 (24%)]\tLoss: 1.928155\n","Train: [18816/78250 (24%)]\tLoss: 1.927149\n","Train: [18944/78250 (24%)]\tLoss: 1.935128\n","Train: [19072/78250 (24%)]\tLoss: 1.929327\n","Train: [19200/78250 (25%)]\tLoss: 1.930218\n","Train: [19328/78250 (25%)]\tLoss: 1.923111\n","Train: [19456/78250 (25%)]\tLoss: 1.936641\n","Train: [19584/78250 (25%)]\tLoss: 1.913400\n","Train: [19712/78250 (25%)]\tLoss: 1.917628\n","Train: [19840/78250 (25%)]\tLoss: 1.938130\n","Train: [19968/78250 (26%)]\tLoss: 1.908737\n","Train: [20096/78250 (26%)]\tLoss: 1.955812\n","Train: [20224/78250 (26%)]\tLoss: 1.912959\n","Train: [20352/78250 (26%)]\tLoss: 1.927504\n","Train: [20480/78250 (26%)]\tLoss: 1.932425\n","Train: [20608/78250 (26%)]\tLoss: 1.921659\n","Train: [20736/78250 (27%)]\tLoss: 1.924443\n","Train: [20864/78250 (27%)]\tLoss: 1.945000\n","Train: [20992/78250 (27%)]\tLoss: 1.917012\n","Train: [21120/78250 (27%)]\tLoss: 1.937038\n","Train: [21248/78250 (27%)]\tLoss: 1.939402\n","Train: [21376/78250 (27%)]\tLoss: 1.947147\n","Train: [21504/78250 (27%)]\tLoss: 1.947228\n","Train: [21632/78250 (28%)]\tLoss: 1.967852\n","Train: [21760/78250 (28%)]\tLoss: 1.902773\n","Train: [21888/78250 (28%)]\tLoss: 1.932521\n","Train: [22016/78250 (28%)]\tLoss: 1.922467\n","Train: [22144/78250 (28%)]\tLoss: 1.940390\n","Train: [22272/78250 (28%)]\tLoss: 1.934084\n","Train: [22400/78250 (29%)]\tLoss: 1.921391\n","Train: [22528/78250 (29%)]\tLoss: 1.920299\n","Train: [22656/78250 (29%)]\tLoss: 1.905829\n","Train: [22784/78250 (29%)]\tLoss: 1.932955\n","Train: [22912/78250 (29%)]\tLoss: 1.909235\n","Train: [23040/78250 (29%)]\tLoss: 1.911128\n","Train: [23168/78250 (30%)]\tLoss: 1.926951\n","Train: [23296/78250 (30%)]\tLoss: 1.928856\n","Train: [23424/78250 (30%)]\tLoss: 1.929180\n","Train: [23552/78250 (30%)]\tLoss: 1.927761\n","Train: [23680/78250 (30%)]\tLoss: 1.930931\n","Train: [23808/78250 (30%)]\tLoss: 1.938322\n","Train: [23936/78250 (31%)]\tLoss: 1.940996\n","Train: [24064/78250 (31%)]\tLoss: 1.911717\n","Train: [24192/78250 (31%)]\tLoss: 1.900570\n","Train: [24320/78250 (31%)]\tLoss: 1.962278\n","Train: [24448/78250 (31%)]\tLoss: 1.912340\n","Train: [24576/78250 (31%)]\tLoss: 1.951618\n","Train: [24704/78250 (32%)]\tLoss: 1.924221\n","Train: [24832/78250 (32%)]\tLoss: 1.916415\n","Train: [24960/78250 (32%)]\tLoss: 1.917091\n","Train: [25088/78250 (32%)]\tLoss: 1.922431\n","Train: [25216/78250 (32%)]\tLoss: 1.929785\n","Train: [25344/78250 (32%)]\tLoss: 1.914578\n","Train: [25472/78250 (33%)]\tLoss: 1.949757\n","Train: [25600/78250 (33%)]\tLoss: 1.922350\n","Train: [25728/78250 (33%)]\tLoss: 1.925593\n","Train: [25856/78250 (33%)]\tLoss: 1.951033\n","Train: [25984/78250 (33%)]\tLoss: 1.935488\n","Train: [26112/78250 (33%)]\tLoss: 1.926603\n","Train: [26240/78250 (34%)]\tLoss: 1.939053\n","Train: [26368/78250 (34%)]\tLoss: 1.923959\n","Train: [26496/78250 (34%)]\tLoss: 1.928828\n","Train: [26624/78250 (34%)]\tLoss: 1.918780\n","Train: [26752/78250 (34%)]\tLoss: 1.922462\n","Train: [26880/78250 (34%)]\tLoss: 1.914440\n","Train: [27008/78250 (35%)]\tLoss: 1.917831\n","Train: [27136/78250 (35%)]\tLoss: 1.922689\n","Train: [27264/78250 (35%)]\tLoss: 1.959965\n","Train: [27392/78250 (35%)]\tLoss: 1.938604\n","Train: [27520/78250 (35%)]\tLoss: 1.929177\n","Train: [27648/78250 (35%)]\tLoss: 1.936283\n","Train: [27776/78250 (36%)]\tLoss: 1.919402\n","Train: [27904/78250 (36%)]\tLoss: 1.914477\n","Train: [28032/78250 (36%)]\tLoss: 1.918341\n","Train: [28160/78250 (36%)]\tLoss: 1.930095\n","Train: [28288/78250 (36%)]\tLoss: 1.913649\n","Train: [28416/78250 (36%)]\tLoss: 1.925235\n","Train: [28544/78250 (36%)]\tLoss: 1.918759\n","Train: [28672/78250 (37%)]\tLoss: 1.932023\n","Train: [28800/78250 (37%)]\tLoss: 1.918870\n","Train: [28928/78250 (37%)]\tLoss: 1.937741\n","Train: [29056/78250 (37%)]\tLoss: 1.942928\n","Train: [29184/78250 (37%)]\tLoss: 1.941395\n","Train: [29312/78250 (37%)]\tLoss: 1.920722\n","Train: [29440/78250 (38%)]\tLoss: 1.918583\n","Train: [29568/78250 (38%)]\tLoss: 1.915479\n","Train: [29696/78250 (38%)]\tLoss: 1.913150\n","Train: [29824/78250 (38%)]\tLoss: 1.929172\n","Train: [29952/78250 (38%)]\tLoss: 1.923830\n","Train: [30080/78250 (38%)]\tLoss: 1.944872\n","Train: [30208/78250 (39%)]\tLoss: 1.943175\n","Train: [30336/78250 (39%)]\tLoss: 1.931451\n","Train: [30464/78250 (39%)]\tLoss: 1.972568\n","Train: [30592/78250 (39%)]\tLoss: 1.925738\n","Train: [30720/78250 (39%)]\tLoss: 1.902209\n","Train: [30848/78250 (39%)]\tLoss: 1.920170\n","Train: [30976/78250 (40%)]\tLoss: 1.924072\n","Train: [31104/78250 (40%)]\tLoss: 1.904668\n","Train: [31232/78250 (40%)]\tLoss: 1.936295\n","Train: [31360/78250 (40%)]\tLoss: 1.916753\n","Train: [31488/78250 (40%)]\tLoss: 1.921460\n","Train: [31616/78250 (40%)]\tLoss: 1.907103\n","Train: [31744/78250 (41%)]\tLoss: 1.916556\n","Train: [31872/78250 (41%)]\tLoss: 1.924004\n","Train: [32000/78250 (41%)]\tLoss: 1.916909\n","Train: [32128/78250 (41%)]\tLoss: 1.936796\n","Train: [32256/78250 (41%)]\tLoss: 1.941719\n","Train: [32384/78250 (41%)]\tLoss: 1.931896\n","Train: [32512/78250 (42%)]\tLoss: 1.920059\n","Train: [32640/78250 (42%)]\tLoss: 1.964327\n","Train: [32768/78250 (42%)]\tLoss: 1.944809\n","Train: [32896/78250 (42%)]\tLoss: 1.916765\n","Train: [33024/78250 (42%)]\tLoss: 1.929908\n","Train: [33152/78250 (42%)]\tLoss: 1.924509\n","Train: [33280/78250 (43%)]\tLoss: 1.914648\n","Train: [33408/78250 (43%)]\tLoss: 1.914408\n","Train: [33536/78250 (43%)]\tLoss: 1.939276\n","Train: [33664/78250 (43%)]\tLoss: 1.932875\n","Train: [33792/78250 (43%)]\tLoss: 1.918356\n","Train: [33920/78250 (43%)]\tLoss: 1.946618\n","Train: [34048/78250 (44%)]\tLoss: 1.947324\n","Train: [34176/78250 (44%)]\tLoss: 1.929284\n","Train: [34304/78250 (44%)]\tLoss: 1.944476\n","Train: [34432/78250 (44%)]\tLoss: 1.918808\n","Train: [34560/78250 (44%)]\tLoss: 1.930980\n","Train: [34688/78250 (44%)]\tLoss: 1.933773\n","Train: [34816/78250 (45%)]\tLoss: 1.940808\n","Train: [34944/78250 (45%)]\tLoss: 1.908247\n","Train: [35072/78250 (45%)]\tLoss: 1.923844\n","Train: [35200/78250 (45%)]\tLoss: 1.942738\n","Train: [35328/78250 (45%)]\tLoss: 1.909897\n","Train: [35456/78250 (45%)]\tLoss: 1.921567\n","Train: [35584/78250 (45%)]\tLoss: 1.959968\n","Train: [35712/78250 (46%)]\tLoss: 1.941105\n","Train: [35840/78250 (46%)]\tLoss: 1.916936\n","Train: [35968/78250 (46%)]\tLoss: 1.921331\n","Train: [36096/78250 (46%)]\tLoss: 1.930669\n","Train: [36224/78250 (46%)]\tLoss: 1.923243\n","Train: [36352/78250 (46%)]\tLoss: 1.939270\n","Train: [36480/78250 (47%)]\tLoss: 1.923113\n","Train: [36608/78250 (47%)]\tLoss: 1.901659\n","Train: [36736/78250 (47%)]\tLoss: 1.926141\n","Train: [36864/78250 (47%)]\tLoss: 1.906090\n","Train: [36992/78250 (47%)]\tLoss: 1.914390\n","Train: [37120/78250 (47%)]\tLoss: 1.917448\n","Train: [37248/78250 (48%)]\tLoss: 1.903449\n","Train: [37376/78250 (48%)]\tLoss: 1.918807\n","Train: [37504/78250 (48%)]\tLoss: 1.923495\n","Train: [37632/78250 (48%)]\tLoss: 1.921373\n","Train: [37760/78250 (48%)]\tLoss: 1.908207\n","Train: [37888/78250 (48%)]\tLoss: 1.921361\n","Train: [38016/78250 (49%)]\tLoss: 1.929612\n","Train: [38144/78250 (49%)]\tLoss: 1.918590\n","Train: [38272/78250 (49%)]\tLoss: 1.925274\n","Train: [38400/78250 (49%)]\tLoss: 1.937781\n","Train: [38528/78250 (49%)]\tLoss: 1.921999\n","Train: [38656/78250 (49%)]\tLoss: 1.934390\n","Train: [38784/78250 (50%)]\tLoss: 1.921274\n","Train: [38912/78250 (50%)]\tLoss: 1.915962\n","Train: [39040/78250 (50%)]\tLoss: 1.920219\n","Train: [39168/78250 (50%)]\tLoss: 1.926246\n","Train: [39296/78250 (50%)]\tLoss: 1.930929\n","Train: [39424/78250 (50%)]\tLoss: 1.947713\n","Train: [39552/78250 (51%)]\tLoss: 1.899705\n","Train: [39680/78250 (51%)]\tLoss: 1.919522\n","Train: [39808/78250 (51%)]\tLoss: 1.933645\n","Train: [39936/78250 (51%)]\tLoss: 1.954946\n","Train: [40064/78250 (51%)]\tLoss: 1.922320\n","Train: [40192/78250 (51%)]\tLoss: 1.935016\n","Train: [40320/78250 (52%)]\tLoss: 1.926179\n","Train: [40448/78250 (52%)]\tLoss: 1.911764\n","Train: [40576/78250 (52%)]\tLoss: 1.919201\n","Train: [40704/78250 (52%)]\tLoss: 1.949969\n","Train: [40832/78250 (52%)]\tLoss: 1.914724\n","Train: [40960/78250 (52%)]\tLoss: 1.912957\n","Train: [41088/78250 (53%)]\tLoss: 1.923844\n","Train: [41216/78250 (53%)]\tLoss: 1.898295\n","Train: [41344/78250 (53%)]\tLoss: 1.920918\n","Train: [41472/78250 (53%)]\tLoss: 1.931926\n","Train: [41600/78250 (53%)]\tLoss: 1.924789\n","Train: [41728/78250 (53%)]\tLoss: 1.920466\n","Train: [41856/78250 (54%)]\tLoss: 1.910042\n","Train: [41984/78250 (54%)]\tLoss: 1.922937\n","Train: [42112/78250 (54%)]\tLoss: 1.956407\n","Train: [42240/78250 (54%)]\tLoss: 1.912674\n","Train: [42368/78250 (54%)]\tLoss: 1.915173\n","Train: [42496/78250 (54%)]\tLoss: 1.922576\n","Train: [42624/78250 (55%)]\tLoss: 1.919844\n","Train: [42752/78250 (55%)]\tLoss: 1.919232\n","Train: [42880/78250 (55%)]\tLoss: 1.948981\n","Train: [43008/78250 (55%)]\tLoss: 1.915761\n","Train: [43136/78250 (55%)]\tLoss: 1.939413\n","Train: [43264/78250 (55%)]\tLoss: 1.948747\n","Train: [43392/78250 (55%)]\tLoss: 1.904624\n","Train: [43520/78250 (56%)]\tLoss: 1.925631\n","Train: [43648/78250 (56%)]\tLoss: 1.897431\n","Train: [43776/78250 (56%)]\tLoss: 1.910619\n","Train: [43904/78250 (56%)]\tLoss: 1.926873\n","Train: [44032/78250 (56%)]\tLoss: 1.921221\n","Train: [44160/78250 (56%)]\tLoss: 1.922237\n","Train: [44288/78250 (57%)]\tLoss: 1.934330\n","Train: [44416/78250 (57%)]\tLoss: 1.922976\n","Train: [44544/78250 (57%)]\tLoss: 1.938182\n","Train: [44672/78250 (57%)]\tLoss: 1.914584\n","Train: [44800/78250 (57%)]\tLoss: 1.905502\n","Train: [44928/78250 (57%)]\tLoss: 1.923226\n","Train: [45056/78250 (58%)]\tLoss: 1.912249\n","Train: [45184/78250 (58%)]\tLoss: 1.909554\n","Train: [45312/78250 (58%)]\tLoss: 1.925924\n","Train: [45440/78250 (58%)]\tLoss: 1.928615\n","Train: [45568/78250 (58%)]\tLoss: 1.910536\n","Train: [45696/78250 (58%)]\tLoss: 1.924148\n","Train: [45824/78250 (59%)]\tLoss: 1.909546\n","Train: [45952/78250 (59%)]\tLoss: 1.900726\n","Train: [46080/78250 (59%)]\tLoss: 1.943785\n","Train: [46208/78250 (59%)]\tLoss: 1.927450\n","Train: [46336/78250 (59%)]\tLoss: 1.924422\n","Train: [46464/78250 (59%)]\tLoss: 1.925588\n","Train: [46592/78250 (60%)]\tLoss: 1.921875\n","Train: [46720/78250 (60%)]\tLoss: 1.919030\n","Train: [46848/78250 (60%)]\tLoss: 1.942961\n","Train: [46976/78250 (60%)]\tLoss: 1.920328\n","Train: [47104/78250 (60%)]\tLoss: 1.901667\n","Train: [47232/78250 (60%)]\tLoss: 1.927545\n","Train: [47360/78250 (61%)]\tLoss: 1.950766\n","Train: [47488/78250 (61%)]\tLoss: 1.920590\n","Train: [47616/78250 (61%)]\tLoss: 1.914810\n","Train: [47744/78250 (61%)]\tLoss: 1.937824\n","Train: [47872/78250 (61%)]\tLoss: 1.917035\n","Train: [48000/78250 (61%)]\tLoss: 1.921353\n","Train: [48128/78250 (62%)]\tLoss: 1.974371\n","Train: [48256/78250 (62%)]\tLoss: 1.916187\n","Train: [48384/78250 (62%)]\tLoss: 1.938014\n","Train: [48512/78250 (62%)]\tLoss: 1.913022\n","Train: [48640/78250 (62%)]\tLoss: 1.932909\n","Train: [48768/78250 (62%)]\tLoss: 1.913940\n","Train: [48896/78250 (63%)]\tLoss: 1.910155\n","Train: [49024/78250 (63%)]\tLoss: 1.917849\n","Train: [49152/78250 (63%)]\tLoss: 1.931307\n","Train: [49280/78250 (63%)]\tLoss: 1.937684\n","Train: [49408/78250 (63%)]\tLoss: 1.923193\n","Train: [49536/78250 (63%)]\tLoss: 1.929486\n","Train: [49664/78250 (64%)]\tLoss: 1.920218\n","Train: [49792/78250 (64%)]\tLoss: 1.920261\n","Train: [49920/78250 (64%)]\tLoss: 1.920091\n","Train: [50048/78250 (64%)]\tLoss: 1.938587\n","Train: [50176/78250 (64%)]\tLoss: 1.908677\n","Train: [50304/78250 (64%)]\tLoss: 1.938863\n","Train: [50432/78250 (64%)]\tLoss: 1.918900\n","Train: [50560/78250 (65%)]\tLoss: 1.902747\n","Train: [50688/78250 (65%)]\tLoss: 1.918474\n","Train: [50816/78250 (65%)]\tLoss: 1.901344\n","Train: [50944/78250 (65%)]\tLoss: 1.919955\n","Train: [51072/78250 (65%)]\tLoss: 1.921516\n","Train: [51200/78250 (65%)]\tLoss: 1.932627\n","Train: [51328/78250 (66%)]\tLoss: 1.924977\n","Train: [51456/78250 (66%)]\tLoss: 1.938876\n","Train: [51584/78250 (66%)]\tLoss: 1.936420\n","Train: [51712/78250 (66%)]\tLoss: 1.905317\n","Train: [51840/78250 (66%)]\tLoss: 1.926505\n","Train: [51968/78250 (66%)]\tLoss: 1.918966\n","Train: [52096/78250 (67%)]\tLoss: 1.929534\n","Train: [52224/78250 (67%)]\tLoss: 1.955037\n","Train: [52352/78250 (67%)]\tLoss: 1.949945\n","Train: [52480/78250 (67%)]\tLoss: 1.920222\n","Train: [52608/78250 (67%)]\tLoss: 1.911406\n","Train: [52736/78250 (67%)]\tLoss: 1.884681\n","Train: [52864/78250 (68%)]\tLoss: 1.895579\n","Train: [52992/78250 (68%)]\tLoss: 1.950131\n","Train: [53120/78250 (68%)]\tLoss: 1.942869\n","Train: [53248/78250 (68%)]\tLoss: 1.901877\n","Train: [53376/78250 (68%)]\tLoss: 1.937254\n","Train: [53504/78250 (68%)]\tLoss: 1.915783\n","Train: [53632/78250 (69%)]\tLoss: 1.898735\n","Train: [53760/78250 (69%)]\tLoss: 1.916345\n","Train: [53888/78250 (69%)]\tLoss: 1.912462\n","Train: [54016/78250 (69%)]\tLoss: 1.927595\n","Train: [54144/78250 (69%)]\tLoss: 1.904115\n","Train: [54272/78250 (69%)]\tLoss: 1.928077\n","Train: [54400/78250 (70%)]\tLoss: 1.898527\n","Train: [54528/78250 (70%)]\tLoss: 1.942679\n","Train: [54656/78250 (70%)]\tLoss: 1.907933\n","Train: [54784/78250 (70%)]\tLoss: 1.906804\n","Train: [54912/78250 (70%)]\tLoss: 1.920523\n","Train: [55040/78250 (70%)]\tLoss: 1.917814\n","Train: [55168/78250 (71%)]\tLoss: 1.913738\n","Train: [55296/78250 (71%)]\tLoss: 1.940892\n","Train: [55424/78250 (71%)]\tLoss: 1.925307\n","Train: [55552/78250 (71%)]\tLoss: 1.932917\n","Train: [55680/78250 (71%)]\tLoss: 1.935778\n","Train: [55808/78250 (71%)]\tLoss: 1.952068\n","Train: [55936/78250 (72%)]\tLoss: 1.909876\n","Train: [56064/78250 (72%)]\tLoss: 1.917860\n","Train: [56192/78250 (72%)]\tLoss: 1.932122\n","Train: [56320/78250 (72%)]\tLoss: 1.921667\n","Train: [56448/78250 (72%)]\tLoss: 1.934626\n","Train: [56576/78250 (72%)]\tLoss: 1.909888\n","Train: [56704/78250 (73%)]\tLoss: 1.916430\n","Train: [56832/78250 (73%)]\tLoss: 1.913043\n","Train: [56960/78250 (73%)]\tLoss: 1.917189\n","Train: [57088/78250 (73%)]\tLoss: 1.945134\n","Train: [57216/78250 (73%)]\tLoss: 1.907203\n","Train: [57344/78250 (73%)]\tLoss: 1.926433\n","Train: [57472/78250 (73%)]\tLoss: 1.919624\n","Train: [57600/78250 (74%)]\tLoss: 1.939541\n","Train: [57728/78250 (74%)]\tLoss: 1.917799\n","Train: [57856/78250 (74%)]\tLoss: 1.950749\n","Train: [57984/78250 (74%)]\tLoss: 1.939892\n","Train: [58112/78250 (74%)]\tLoss: 1.909223\n","Train: [58240/78250 (74%)]\tLoss: 1.910099\n","Train: [58368/78250 (75%)]\tLoss: 1.931419\n","Train: [58496/78250 (75%)]\tLoss: 1.933093\n","Train: [58624/78250 (75%)]\tLoss: 1.911759\n","Train: [58752/78250 (75%)]\tLoss: 1.978173\n","Train: [58880/78250 (75%)]\tLoss: 1.944242\n","Train: [59008/78250 (75%)]\tLoss: 1.967313\n","Train: [59136/78250 (76%)]\tLoss: 1.918689\n","Train: [59264/78250 (76%)]\tLoss: 1.921617\n","Train: [59392/78250 (76%)]\tLoss: 1.933556\n","Train: [59520/78250 (76%)]\tLoss: 1.909633\n","Train: [59648/78250 (76%)]\tLoss: 1.905896\n","Train: [59776/78250 (76%)]\tLoss: 1.925785\n","Train: [59904/78250 (77%)]\tLoss: 1.918017\n","Train: [60032/78250 (77%)]\tLoss: 1.954373\n","Train: [60160/78250 (77%)]\tLoss: 1.934569\n","Train: [60288/78250 (77%)]\tLoss: 1.937914\n","Train: [60416/78250 (77%)]\tLoss: 1.933875\n","Train: [60544/78250 (77%)]\tLoss: 1.917790\n","Train: [60672/78250 (78%)]\tLoss: 1.911982\n","Train: [60800/78250 (78%)]\tLoss: 1.923763\n","Train: [60928/78250 (78%)]\tLoss: 1.906180\n","Train: [61056/78250 (78%)]\tLoss: 1.928543\n","Train: [61184/78250 (78%)]\tLoss: 1.930828\n","Train: [61312/78250 (78%)]\tLoss: 1.937406\n","Train: [61440/78250 (79%)]\tLoss: 1.936566\n","Train: [61568/78250 (79%)]\tLoss: 1.915254\n","Train: [61696/78250 (79%)]\tLoss: 1.933201\n","Train: [61824/78250 (79%)]\tLoss: 1.923276\n","Train: [61952/78250 (79%)]\tLoss: 1.927185\n","Train: [62080/78250 (79%)]\tLoss: 1.926408\n","Train: [62208/78250 (80%)]\tLoss: 1.923016\n","Train: [62336/78250 (80%)]\tLoss: 1.913786\n","Train: [62464/78250 (80%)]\tLoss: 1.913418\n","Train: [62592/78250 (80%)]\tLoss: 1.904924\n","Train: [62720/78250 (80%)]\tLoss: 1.943294\n","Train: [62848/78250 (80%)]\tLoss: 1.953728\n","Train: [62976/78250 (81%)]\tLoss: 1.915376\n","Train: [63104/78250 (81%)]\tLoss: 1.940452\n","Train: [63232/78250 (81%)]\tLoss: 1.934817\n","Train: [63360/78250 (81%)]\tLoss: 1.933165\n","Train: [63488/78250 (81%)]\tLoss: 1.922586\n","Train: [63616/78250 (81%)]\tLoss: 1.941242\n","Train: [63744/78250 (82%)]\tLoss: 1.921551\n","Train: [63872/78250 (82%)]\tLoss: 1.945000\n","Train: [64000/78250 (82%)]\tLoss: 1.908154\n","Train: [64128/78250 (82%)]\tLoss: 1.926964\n","Train: [64256/78250 (82%)]\tLoss: 1.905505\n","Train: [64384/78250 (82%)]\tLoss: 1.909851\n","Train: [64512/78250 (82%)]\tLoss: 1.929836\n","Train: [64640/78250 (83%)]\tLoss: 1.948097\n","Train: [64768/78250 (83%)]\tLoss: 1.916320\n","Train: [64896/78250 (83%)]\tLoss: 1.916926\n","Train: [65024/78250 (83%)]\tLoss: 1.940100\n","Train: [65152/78250 (83%)]\tLoss: 1.909983\n","Train: [65280/78250 (83%)]\tLoss: 1.912150\n","Train: [65408/78250 (84%)]\tLoss: 1.919455\n","Train: [65536/78250 (84%)]\tLoss: 1.909165\n","Train: [65664/78250 (84%)]\tLoss: 1.932573\n","Train: [65792/78250 (84%)]\tLoss: 1.911058\n","Train: [65920/78250 (84%)]\tLoss: 1.914778\n","Train: [66048/78250 (84%)]\tLoss: 1.938082\n","Train: [66176/78250 (85%)]\tLoss: 1.913363\n","Train: [66304/78250 (85%)]\tLoss: 1.909148\n","Train: [66432/78250 (85%)]\tLoss: 1.921964\n","Train: [66560/78250 (85%)]\tLoss: 1.943573\n","Train: [66688/78250 (85%)]\tLoss: 1.924129\n","Train: [66816/78250 (85%)]\tLoss: 1.917183\n","Train: [66944/78250 (86%)]\tLoss: 1.926504\n","Train: [67072/78250 (86%)]\tLoss: 1.918372\n","Train: [67200/78250 (86%)]\tLoss: 1.950753\n","Train: [67328/78250 (86%)]\tLoss: 1.903158\n","Train: [67456/78250 (86%)]\tLoss: 1.928267\n","Train: [67584/78250 (86%)]\tLoss: 1.930355\n","Train: [67712/78250 (87%)]\tLoss: 1.903127\n","Train: [67840/78250 (87%)]\tLoss: 1.940888\n","Train: [67968/78250 (87%)]\tLoss: 1.920460\n","Train: [68096/78250 (87%)]\tLoss: 1.940304\n","Train: [68224/78250 (87%)]\tLoss: 1.940618\n","Train: [68352/78250 (87%)]\tLoss: 1.943625\n","Train: [68480/78250 (88%)]\tLoss: 1.912598\n","Train: [68608/78250 (88%)]\tLoss: 1.950060\n","Train: [68736/78250 (88%)]\tLoss: 1.895304\n","Train: [68864/78250 (88%)]\tLoss: 1.937253\n","Train: [68992/78250 (88%)]\tLoss: 1.925945\n","Train: [69120/78250 (88%)]\tLoss: 1.942839\n","Train: [69248/78250 (89%)]\tLoss: 1.918438\n"]}],"source":["import sys\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms \n","import torch.optim as optim\n","from torch.optim import lr_scheduler \n","from ABE_M_model import ABE_M \n","from my_model import se_resnet50\n","from resnet import resnet50\n","from dataset import SingleData\n","from torch.utils.data import DataLoader\n","from loss_func import ABE_loss, ContrastiveLoss, Ms_loss\n","from torchsummary import summary\n","\n","from torchvision.transforms import InterpolationMode\n","\n","def get_data_list(data_path, ratio=0.1):\n","    img_list = []\n","    for root, dirs, files in os.walk(data_path):\n","        if files == []:\n","            class_name = dirs\n","        elif dirs == []:\n","            for f in files:\n","                img_path = os.path.join(root, f)\n","                img_list.append(img_path)\n","\n","    np.random.seed(1)\n","    train_img_list = np.random.choice(img_list, size=int(len(img_list)*(1-ratio)), replace=False)\n","    #print(img_list, train_img_list)\n","    eval_img_list = list(set(img_list) - set(train_img_list))\n","    print(data_path)\n","    return class_name, train_img_list, eval_img_list \n","\n","def train_epoch(train_loader, model, loss_fn, optimizer, device):\n","    model.train()\n","    losses = []\n","    total_loss = 0\n","    for batch_idx, (data, target, _) in enumerate(train_loader):\n","        data = data.to(device)\n","        target = target.to(device)\n","\n","        optimizer.zero_grad()\n","        output = model(data)\n","        target = target\n","\n","        loss_outputs = loss_fn(output, target)\n","\n","        losses.append(loss_outputs.item())\n","        total_loss += loss_outputs.item()\n","        if loss_outputs.requires_grad is True:\n","            loss_outputs.backward()\n","            optimizer.step()\n","\n","        if batch_idx % 2 == 0:\n","            message = 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(batch_idx * target.size(0), len(train_loader.dataset), 100. * batch_idx / len(train_loader), np.mean(losses))\n","            print(message)\n","            losses = []\n","\n","    print('total loss {:.6f}'.format(total_loss/(batch_idx+1)))\n","\n","\n","def eval_epoch(eval_loader, model, loss_fn, device, best, model_name, loss_name):\n","    with torch.no_grad():\n","        model.eval()\n","        val_loss = 0\n","        for batch_idx, (data, target, _) in enumerate(eval_loader):\n","            data = data.to(device)\n","            target = target.to(device)\n","\n","            output = model(data)\n","\n","            loss_outputs = loss_fn(output, target)\n","            #print(loss_outputs)\n","            val_loss += loss_outputs.item()\n","\n","    print('val loss {:.6f}'.format(val_loss/(batch_idx+1)))\n","    if best \u003e val_loss/(batch_idx+1):\n","        best = val_loss/(batch_idx+1)\n","        if torch.cuda.device_count() \u003e 1:\n","            torch.save(model.module.state_dict(), '/content/drive/MyDrive/models/MA/bracs/{}_{}_{:.4f}.pth'.format(model_name, loss_name, best))\n","        else:\n","            torch.save(model.state_dict(), '/content/drive/MyDrive/models/MA/bracs/{}_{}_{:.4f}.pth'.format(model_name, loss_name, best))\n","    return best \n","\n","\n","if __name__ == '__main__':\n"," \n","    torch.cuda.empty_cache()\n","    # arg_len = len(sys.argv)\n","    # if arg_len != 3:\n","    #     raise Exception(\"Invalid argvs!\")\n","    # model_name = sys.argv[1]\n","    # loss_name = sys.argv[2]\n","    \n","    model_name = 'se_resnet'\n","    loss_name = 'MsLoss'\n","    print('model is {}, loss function is {}'.format(model_name, loss_name))\n","\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","    # device = torch.device('cpu')\n","    print('device',device)\n","    epochs = 100\n","    batch_size = 64   # 64\n","    lr = 0.001\n","    #num_learner = 4\n","    model_dict = {'ABE_M':ABE_M, 'se_resnet':se_resnet50, 'resnet50':resnet50}\n","    loss_dict = {'ABELoss':ABE_loss, 'ContrastiveLoss':ContrastiveLoss, 'MsLoss':Ms_loss}\n","\n","\n","    #model = model_dict[model_name](attention=False) #################### remove attention\n","    model = model_dict[model_name]()\n","    print(torch.cuda.device_count())\n","    if torch.cuda.device_count() \u003e 1:\n","        print(\"Let's use {} GPUs!\".format(torch.cuda.device_count()))\n","        model = nn.DataParallel(model)\n","    model.load_state_dict(torch.load('/content/drive/MyDrive/models/MA/bracs/se_resnet_MsLoss_1.9331.pth'))\n","    model.to(device)\n","    \n","    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n","    #optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    loss_fn = loss_dict[loss_name]() ########################################\n","\n","    scheduler = lr_scheduler.StepLR(optimizer, step_size=50)\n","    \n","    #data_path = 'D:\\\\University\\\\training-\\\\train'\n","    data_path = '/content/drive/MyDrive/datasets/bracs'\n","    \n","    class_name, train_img_list, eval_img_list = get_data_list(data_path)\n","\n","    train_transform = transforms.Compose([ \n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomVerticalFlip(),\n","        transforms.RandomCrop(224),\n","        transforms.ToTensor()\n","    ])\n","    eval_transform = transforms.Compose([\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor()\n","    ])\n","    train_transform2 = transforms.Compose([ \n","                                          transforms.Resize((224,224),interpolation=InterpolationMode.BICUBIC),\n","                                          transforms.ColorJitter(brightness=.5, saturation=.25, hue=.1, contrast=.5),\n","                                          transforms.RandomAffine(10, (0.05, 0.05), fill=(255, 255, 255)),\n","                                          transforms.RandomHorizontalFlip(.5),\n","                                          transforms.RandomVerticalFlip(.5),\n","                                         transforms.ToTensor()])\n","    \n","    #========================== single data ================================\n","    torch.cuda.empty_cache()\n","    train_dataset = SingleData(class_name, train_img_list, train_transform)\n","    eval_dataset = SingleData(class_name, eval_img_list, eval_transform)\n","    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","    eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","    \n","    best = 10000\n","    for epoch in range(epochs):\n","        torch.cuda.empty_cache()\n","        print('epoch {}/{}'.format(epoch, epochs))\n","        \n","        train_epoch(train_dataloader, model, loss_fn, optimizer, device)\n","        scheduler.step() \n","        # with torch.no_grad():\n","        best = eval_epoch(eval_dataloader, model, loss_fn, device, best, model_name, loss_name)\n","\n","        \n","\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPPX4h2bYyqXbJQciV2Yto0","name":"Untitled1.ipynb","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}